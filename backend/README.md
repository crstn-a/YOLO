# YOLOv11 Fingerspelling Web App & Benchmark Framework

A comprehensive web application and benchmark framework for evaluating YOLOv11's accuracy and performance in recognizing ASL fingerspelling letters, including both static (A-I, K-Y) and dynamic (J, Z) gestures. This project combines a Flutter frontend with a FastAPI backend to create an interactive fingerspelling recognition system.

## ğŸŒŸ What You Can Expect

### **Interactive Web Application**
- **Real-time Camera Interface**: Capture and analyze fingerspelling gestures through your webcam
- **Live Recognition**: Get instant feedback on static letters (A-I, K-Y) and dynamic gestures (J, Z)
- **Performance Metrics**: See real-time FPS, latency, and accuracy measurements
- **Cross-platform Support**: Works on desktop, mobile, and web browsers

### **Comprehensive Benchmark Suite**
- **Static Accuracy Testing**: Evaluate recognition of 24 static letters with precision, recall, and F1-score
- **Dynamic Gesture Analysis**: Test J and Z gestures using Dynamic Time Warping (DTW)
- **Real-time Performance**: Measure FPS and latency for deployment readiness
- **Research Grade Assessment**: Determine if your model meets research-grade deployment criteria

### **Professional Reporting**
- **Automated PDF Reports**: Generate comprehensive benchmark reports
- **Visual Analytics**: Charts and tables showing performance metrics
- **Research Documentation**: Detailed analysis for academic and commercial use

## ğŸ¯ Objectives

This framework evaluates:
- **Static Accuracy**: Recognition of static fingerspelling letters (A-I, K-Y) using standard object detection metrics
- **Dynamic Gestures**: Recognition of dynamic letters (J, Z) using Dynamic Time Warping (DTW) and sequence-level evaluation
- **Real-time Performance**: FPS and latency measurements for deployment readiness
- **Research Grade Assessment**: Comprehensive evaluation against research-grade deployment criteria

## ğŸ† Research Grade Criteria

A model is considered **research-grade and deployment-ready** if it meets:
- **Static Accuracy**: â‰¥ 90% on static letters (A-I, K-Y)
- **Dynamic Accuracy**: â‰¥ 85% on dynamic gestures (J, Z)
- **Real-time Performance**: â‰¥ 25 FPS
- **DTW Similarity**: â‰¥ 70% for dynamic gesture recognition

## ğŸ“ Complete Project Structure

```
YOLO/                          # Root project directory
â”œâ”€â”€ backend/                   # Python FastAPI backend
â”‚   â”œâ”€â”€ config.py             # Configuration management
â”‚   â”œâ”€â”€ main.py               # FastAPI application server
â”‚   â”œâ”€â”€ requirements.txt      # Python dependencies
â”‚   â”œâ”€â”€ README.md             # This documentation
â”‚   â”œâ”€â”€ test_framework.py     # Framework testing script
â”‚   â”œâ”€â”€ example_usage.py      # Usage examples
â”‚   â”œâ”€â”€ benchmarks/
â”‚   â”‚   â””â”€â”€ benchmark_runner.py  # Command-line benchmark runner
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ detection.py      # YOLOv11 detection module
â”‚   â”‚   â”œâ”€â”€ dtw_evaluation.py # Dynamic gesture evaluation
â”‚   â”‚   â”œâ”€â”€ benchmarking.py   # Comprehensive benchmark suite
â”‚   â”‚   â”œâ”€â”€ metrics.py        # Metrics calculation
â”‚   â”‚   â””â”€â”€ report_generator.py # PDF report generation
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ download_model.py # Model download script
â”‚   â”‚   â””â”€â”€ yolov11_fingerspelling.pt  # YOLOv11 model (download required)
â”‚   â”œâ”€â”€ datasets/
â”‚   â”‚   â””â”€â”€ test_data/        # Test dataset directory (create your own)
â”‚   â””â”€â”€ results/
â”‚       â”œâ”€â”€ logs/             # JSON log files (auto-generated)
â”‚       â””â”€â”€ reports/          # PDF reports (auto-generated)
â””â”€â”€ frontend/                 # Flutter web/mobile app
    â”œâ”€â”€ lib/
    â”‚   â”œâ”€â”€ main.dart         # Flutter app entry point
    â”‚   â”œâ”€â”€ pages/
    â”‚   â”‚   â””â”€â”€ camera_page.dart  # Camera interface
    â”‚   â””â”€â”€ services/
    â”‚       â””â”€â”€ api_services.dart # API communication
    â”œâ”€â”€ pubspec.yaml          # Flutter dependencies
    â”œâ”€â”€ android/              # Android-specific files
    â”œâ”€â”€ ios/                  # iOS-specific files
    â”œâ”€â”€ web/                  # Web-specific files
    â””â”€â”€ windows/              # Windows-specific files
```

## ğŸ¯ What You'll Find in This Project

### **Backend Components (Python/FastAPI)**
- **ğŸ¤– AI Model Integration**: YOLOv11 for fingerspelling recognition
- **ğŸ“Š Benchmark Framework**: Comprehensive evaluation suite
- **ğŸ”§ API Server**: RESTful endpoints for web/mobile apps
- **ğŸ“ˆ Performance Metrics**: Real-time FPS and accuracy measurement
- **ğŸ“„ Automated Reporting**: PDF generation with visual analytics

### **Frontend Components (Flutter)**
- **ğŸ“± Cross-Platform App**: Works on web, mobile, and desktop
- **ğŸ“· Camera Interface**: Real-time gesture capture and recognition
- **ğŸ¨ Modern UI**: Clean, responsive design for all devices
- **âš¡ Live Feedback**: Instant recognition results and performance metrics
- **ğŸ”„ Real-time Updates**: Dynamic gesture tracking and analysis

### **Key Features You Can Explore**

#### **ğŸ”¬ Research & Development**
- **Model Evaluation**: Test your own YOLOv11 models
- **Dataset Benchmarking**: Evaluate performance on custom datasets
- **Performance Analysis**: Detailed FPS, latency, and accuracy metrics
- **Research Reports**: Publication-ready PDF reports with visualizations

#### **ğŸ® Interactive Applications**
- **Real-time Recognition**: Live fingerspelling detection through camera
- **Gesture Learning**: Practice and improve ASL fingerspelling
- **Performance Monitoring**: See real-time system performance
- **Cross-platform Access**: Use on any device with a camera

#### **ğŸ› ï¸ Developer Tools**
- **API Documentation**: Interactive API testing interface
- **Command-line Tools**: Automated benchmark execution
- **Configuration Management**: Customizable parameters and thresholds
- **Logging & Debugging**: Comprehensive error tracking and diagnostics

### **What Makes This Project Special**

#### **ğŸ¯ Research-Grade Evaluation**
- **Rigorous Metrics**: Industry-standard evaluation methods
- **Comprehensive Testing**: Static, dynamic, and performance benchmarks
- **Academic Standards**: Publication-ready analysis and reporting
- **Reproducible Results**: Automated testing with consistent methodology

#### **ğŸš€ Production-Ready Architecture**
- **Scalable Backend**: FastAPI with async processing
- **Modern Frontend**: Flutter for cross-platform compatibility
- **Cloud Deployment**: Ready for AWS, GCP, or Azure deployment
- **API-First Design**: Easy integration with other applications

#### **ğŸ“š Educational Value**
- **Learning Resource**: Understand computer vision and gesture recognition
- **Code Examples**: Well-documented, production-quality code
- **Best Practices**: Industry-standard development patterns
- **Open Source**: Contribute and learn from the community

### **Target Audiences**

#### **ğŸ‘¨â€ğŸ“ Researchers & Academics**
- **Gesture Recognition Research**: State-of-the-art evaluation framework
- **ASL Studies**: Comprehensive fingerspelling analysis tools
- **Computer Vision**: YOLOv11 implementation and benchmarking
- **Publication Support**: Automated report generation for papers

#### **ğŸ‘¨â€ğŸ’» Developers & Engineers**
- **AI Integration**: Learn to integrate YOLOv11 in applications
- **Web Development**: FastAPI and Flutter best practices
- **API Design**: RESTful service architecture
- **Performance Optimization**: Real-time inference optimization

#### **ğŸ“ Students & Learners**
- **Computer Vision**: Hands-on experience with object detection
- **Machine Learning**: Understanding model evaluation and metrics
- **Web Development**: Full-stack application development
- **Research Methods**: Scientific evaluation and reporting

#### **ğŸ¢ Industry Professionals**
- **Accessibility Tools**: ASL recognition for inclusive applications
- **Gesture Interfaces**: Natural user interaction systems
- **Performance Testing**: Benchmarking AI model deployment
- **Research & Development**: Rapid prototyping and evaluation

## ğŸš€ Quick Start Options

### **ğŸ¯ For Different User Types**

#### **ğŸ‘¨â€ğŸ’» Developers - Quick API Test**
```bash
# 1. Setup backend only
cd backend
pip install -r requirements.txt
python main.py

# 2. Test API at http://localhost:8000
# 3. Use interactive API docs to test endpoints
```

#### **ğŸ“ Students - Learn & Experiment**
```bash
# 1. Full setup with frontend
cd backend && pip install -r requirements.txt
cd ../frontend && flutter pub get

# 2. Run both backend and frontend
# 3. Experiment with different gestures
# 4. Analyze performance metrics
```

#### **ğŸ‘¨â€ğŸ“ Researchers - Benchmark Focus**
```bash
# 1. Setup backend with dataset
cd backend
pip install -r requirements.txt

# 2. Prepare your dataset in datasets/test_data/
# 3. Run comprehensive benchmarks
python benchmarks/benchmark_runner.py --mode comprehensive

# 4. Analyze PDF reports in results/reports/
```

#### **ğŸ¢ Industry - Production Deployment**
```bash
# 1. Full production setup
# 2. Configure for your specific use case
# 3. Deploy to cloud platforms
# 4. Integrate with existing systems
```

## ğŸš€ Complete Setup Guide

### **Prerequisites**
- Python 3.8+ installed
- Flutter SDK installed (for frontend)
- Webcam or camera device
- At least 4GB RAM (8GB+ recommended for optimal performance)
- GPU support (optional but recommended for better performance)

### **Step 1: Clone and Setup Backend**

```bash
# Clone the repository (if not already done)
git clone <your-repo-url>
cd YOLO

# Navigate to backend directory
cd backend

# Install Python dependencies
pip install -r requirements.txt

# Test the framework
python test_framework.py
```

### **Step 2: Download YOLOv11 Model**

```bash
# Create models directory if it doesn't exist
mkdir -p models

# Download the YOLOv11 fingerspelling model
# Option 1: Use the download script (if available)
python models/download_model.py

# Option 2: Manually download and place yolov11_fingerspelling.pt in models/ directory
# You can train your own model or use a pre-trained one

# ğŸ“š For detailed dataset and model information, see:
# DATASETS_AND_MODELS.md - Complete guide to datasets and pre-trained models
```

### **Step 3: Prepare Test Dataset (Optional but Recommended)**

```bash
# Create dataset structure
mkdir -p datasets/test_data

# Organize your test images like this:
datasets/test_data/
â”œâ”€â”€ A/                    # Static letter A
â”‚   â”œâ”€â”€ image1.jpg
â”‚   â”œâ”€â”€ image2.jpg
â”‚   â””â”€â”€ ...
â”œâ”€â”€ B/                    # Static letter B
â”‚   â””â”€â”€ ...
â”œâ”€â”€ J/                    # Dynamic letter J
â”‚   â”œâ”€â”€ sequence1/        # J gesture sequence
â”‚   â”‚   â”œâ”€â”€ frame1.jpg
â”‚   â”‚   â”œâ”€â”€ frame2.jpg
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ sequence2/
â”‚       â””â”€â”€ ...
â””â”€â”€ Z/                    # Dynamic letter Z
    â”œâ”€â”€ sequence1/
    â””â”€â”€ ...
```

### **Step 4: Start the Backend API Server**

```bash
# Start FastAPI server
python main.py

# Or with uvicorn for production
uvicorn main:app --host 0.0.0.0 --port 8000 --reload
```

**Expected Output:**
```
INFO:     Started server process [12345]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)
```

### **Step 5: Setup Flutter Frontend**

```bash
# Navigate to frontend directory
cd ../frontend

# Install Flutter dependencies
flutter pub get

# Run the Flutter app
flutter run

# For web deployment:
flutter run -d chrome

# For mobile deployment:
flutter run -d android  # or flutter run -d ios
```

### **Step 6: Access the Web Application**

#### **Option A: Flutter Web App**
1. Open your browser and go to `http://localhost:3000` (or the port shown in Flutter output)
2. Allow camera permissions when prompted
3. Start fingerspelling recognition!

#### **Option B: API Testing**
1. Open `http://localhost:8000` in your browser to see the API documentation
2. Use the interactive API interface to test endpoints
3. Upload images for static letter recognition
4. Upload multiple images for dynamic gesture recognition

### **Step 7: Run Comprehensive Benchmarks**

```bash
# Navigate back to backend directory
cd ../backend

# Run comprehensive benchmark suite
python benchmarks/benchmark_runner.py --mode comprehensive

# Run specific benchmarks
python benchmarks/benchmark_runner.py --mode static
python benchmarks/benchmark_runner.py --mode dynamic
python benchmarks/benchmark_runner.py --mode efficiency
```

**Expected Output:**
```
ğŸš€ Starting Comprehensive YOLOv11 Fingerspelling Benchmark...
============================================================
ğŸ” Running Static Fingerspelling Benchmark...
âœ… Static Benchmark Complete - Accuracy: 0.923
ğŸ”„ Running Dynamic Gesture Benchmark...
âœ… Dynamic Benchmark Complete - Sequence Accuracy: 0.856
âš¡ Running Efficiency Benchmark...
âœ… Efficiency Benchmark Complete - Avg FPS: 28.45
============================================================
ğŸ¯ BENCHMARK SUMMARY
============================================================
ğŸ“Š Static Accuracy: 0.923
ğŸ”„ Dynamic Sequence Accuracy: 0.856
âš¡ Average FPS: 28.45
ğŸ† Research Grade Ready: True
============================================================
```

### **Step 8: View Results and Reports**

```bash
# Check generated reports
ls results/reports/

# Open the latest PDF report
# The reports will be in results/reports/ directory
```

## ğŸ® How to Use the Web Application

### **Real-time Recognition**
1. **Open the Flutter app** in your browser or mobile device
2. **Allow camera access** when prompted
3. **Show fingerspelling gestures** to the camera:
   - **Static letters (A-I, K-Y)**: Hold the gesture steady for 1-2 seconds
   - **Dynamic letters (J, Z)**: Perform the full gesture movement
4. **View results** in real-time with confidence scores and performance metrics

### **Benchmark Testing**
1. **Prepare test images** in the dataset structure
2. **Run benchmarks** using the command-line interface
3. **View comprehensive reports** in PDF format
4. **Analyze performance** against research-grade criteria

## ğŸ”§ Troubleshooting Common Issues

### **Backend Issues**

#### **Model Not Found Error**
```bash
# Check if model exists
ls -la models/yolov11_fingerspelling.pt

# If missing, download or train a model
python models/download_model.py
```

#### **Port Already in Use**
```bash
# Kill existing processes on port 8000
lsof -ti:8000 | xargs kill -9

# Or use a different port
uvicorn main:app --port 8001
```

#### **Memory Issues**
```bash
# Reduce batch size in config.py
# Or use CPU-only mode
export CUDA_VISIBLE_DEVICES=""
```

### **Frontend Issues**

#### **Flutter Dependencies**
```bash
# Clean and reinstall
flutter clean
flutter pub get
```

#### **Camera Permissions**
- **Web**: Ensure HTTPS or localhost for camera access
- **Mobile**: Check app permissions in device settings
- **Desktop**: Allow camera access in browser settings

#### **API Connection Issues**
```bash
# Check if backend is running
curl http://localhost:8000/health/

# Update API URL in Flutter app if needed
# Check lib/services/api_services.dart
```

### **Performance Optimization**

#### **For Better FPS**
```bash
# Use GPU acceleration
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118

# Reduce image resolution in config.py
# Use smaller model variants
```

#### **For Better Accuracy**
```bash
# Increase confidence threshold
# Use higher quality training data
# Fine-tune model on your specific dataset
```

## ğŸ“Š What to Expect: Performance Benchmarks

### **Typical Performance Results**
- **Static Accuracy**: 85-95% (depending on model quality)
- **Dynamic Accuracy**: 80-90% (J and Z gestures)
- **Real-time FPS**: 15-30 FPS (varies by hardware)
- **Latency**: 30-100ms per inference

### **Hardware Requirements**
- **Minimum**: CPU-only, 4GB RAM, webcam
- **Recommended**: GPU, 8GB+ RAM, high-quality camera
- **Optimal**: RTX 3060+ GPU, 16GB+ RAM, 1080p camera

### **Research Grade Criteria**
Your model is considered **research-grade** if it achieves:
- âœ… **Static Accuracy**: â‰¥ 90%
- âœ… **Dynamic Accuracy**: â‰¥ 85%
- âœ… **Real-time Performance**: â‰¥ 25 FPS
- âœ… **DTW Similarity**: â‰¥ 70%

## ğŸ¯ Next Steps After Setup

### **For Researchers**
1. **Collect your own dataset** with diverse lighting and backgrounds
2. **Fine-tune the model** on your specific data
3. **Run comprehensive benchmarks** to validate performance
4. **Generate research reports** for publication

### **For Developers**
1. **Integrate the API** into your own applications
2. **Customize the frontend** for your specific use case
3. **Deploy to cloud platforms** for production use
4. **Add new features** like gesture recording and playback

### **For Students**
1. **Experiment with different models** and parameters
2. **Learn about computer vision** and gesture recognition
3. **Understand benchmark metrics** and evaluation methods
4. **Contribute to the project** with improvements

## ğŸ†˜ Getting Help

### **Common Questions**
- **Q**: The app doesn't recognize my gestures
- **A**: Ensure good lighting, clear hand positioning, and try different angles

- **Q**: Performance is slow
- **A**: Check if GPU is being used, reduce image resolution, or use a smaller model

- **Q**: Benchmark fails with errors
- **A**: Ensure dataset is properly structured and model file exists

### **Support Resources**
- Check the `test_framework.py` script for diagnostics
- Review log files in `results/logs/` directory
- Test individual components using the API endpoints
- Check the Flutter console for frontend errors

## ğŸ¯ Expected Outcomes & Success Metrics

### **What You Should See When Everything Works**

#### **âœ… Successful Backend Startup**
```
INFO:     Started server process [12345]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)
```

#### **âœ… Successful Frontend Launch**
```
Flutter run key commands.
r Hot reload. ğŸ”¥ğŸ”¥ğŸ”¥
R Hot restart.
h List all available interactive commands.
d Detach (terminate "flutter run" but leave application running).
c Clear the screen
q Quit (terminate the application on the device).
```

#### **âœ… Successful Benchmark Run**
```
ğŸš€ Starting Comprehensive YOLOv11 Fingerspelling Benchmark...
============================================================
ğŸ” Running Static Fingerspelling Benchmark...
âœ… Static Benchmark Complete - Accuracy: 0.923
ğŸ”„ Running Dynamic Gesture Benchmark...
âœ… Dynamic Benchmark Complete - Sequence Accuracy: 0.856
âš¡ Running Efficiency Benchmark...
âœ… Efficiency Benchmark Complete - Avg FPS: 28.45
============================================================
ğŸ¯ BENCHMARK SUMMARY
============================================================
ğŸ“Š Static Accuracy: 0.923
ğŸ”„ Dynamic Sequence Accuracy: 0.856
âš¡ Average FPS: 28.45
ğŸ† Research Grade Ready: True
============================================================
```

### **ğŸ¯ Success Indicators**

#### **Backend Health Check**
```bash
curl http://localhost:8000/health/
# Expected: {"status": "healthy", "model_loaded": true, "config_loaded": true}
```

#### **API Documentation**
- Visit `http://localhost:8000` to see interactive API docs
- All endpoints should be listed and testable
- Upload test images should return recognition results

#### **Frontend Functionality**
- Camera permission granted
- Live video feed visible
- Recognition results appear in real-time
- Performance metrics displayed

#### **Benchmark Results**
- PDF reports generated in `results/reports/`
- JSON logs created in `results/logs/`
- Research-grade assessment completed
- All metrics within expected ranges

### **ğŸ“Š Performance Benchmarks to Expect**

#### **Typical Results (Good Model)**
- **Static Accuracy**: 85-95%
- **Dynamic Accuracy**: 80-90%
- **FPS**: 20-35 (varies by hardware)
- **Latency**: 30-80ms per inference

#### **Research-Grade Results**
- **Static Accuracy**: â‰¥ 90%
- **Dynamic Accuracy**: â‰¥ 85%
- **FPS**: â‰¥ 25
- **DTW Similarity**: â‰¥ 70%

#### **Hardware Performance Expectations**
- **CPU Only**: 10-20 FPS, 50-100ms latency
- **GPU (RTX 3060)**: 25-35 FPS, 30-50ms latency
- **GPU (RTX 4090)**: 40-60 FPS, 20-30ms latency

### **ğŸ‰ What Success Looks Like**

#### **For Researchers**
- âœ… Comprehensive benchmark reports generated
- âœ… Research-grade criteria met or exceeded
- âœ… Publication-ready metrics and visualizations
- âœ… Reproducible results with detailed logging

#### **For Developers**
- âœ… API endpoints responding correctly
- âœ… Real-time recognition working smoothly
- âœ… Cross-platform compatibility confirmed
- âœ… Performance metrics within acceptable ranges

#### **For Students**
- âœ… Interactive web app functioning
- âœ… Real-time gesture recognition working
- âœ… Performance monitoring visible
- âœ… Learning objectives achieved

#### **For Industry Professionals**
- âœ… Production-ready architecture
- âœ… Scalable performance metrics
- âœ… Professional reporting system
- âœ… Integration-ready API design

### **ğŸš¨ Common Issues & Solutions**

#### **If Backend Won't Start**
- Check Python version (3.8+ required)
- Verify all dependencies installed
- Check if port 8000 is available
- Ensure model file exists

#### **If Frontend Won't Load**
- Check Flutter installation
- Verify camera permissions
- Check API connection
- Clear Flutter cache if needed

#### **If Recognition is Poor**
- Improve lighting conditions
- Ensure clear hand positioning
- Check model quality
- Adjust confidence thresholds

#### **If Performance is Slow**
- Enable GPU acceleration
- Reduce image resolution
- Use smaller model variant
- Check system resources

### **ğŸ¯ Next Steps After Success**

#### **Immediate Actions**
1. **Test all features** thoroughly
2. **Run comprehensive benchmarks** with your data
3. **Generate reports** and analyze results
4. **Document your findings** for future reference

#### **Long-term Goals**
1. **Collect your own dataset** for better accuracy
2. **Fine-tune the model** for your specific use case
3. **Deploy to production** environment
4. **Contribute improvements** to the project

#### **Advanced Usage**
1. **Customize the UI** for your needs
2. **Integrate with other systems** via API
3. **Scale for multiple users** with cloud deployment
4. **Develop new features** based on your requirements

## ğŸ“Š Benchmark Components

### Static Fingerspelling Evaluation (A-I, K-Y)

Evaluates recognition of static handshapes using:
- **Precision, Recall, F1-Score**: Standard object detection metrics
- **Per-class Accuracy**: Individual letter performance
- **Confusion Matrix**: Detailed error analysis
- **Threshold**: 90% accuracy required for research-grade

### Dynamic Gesture Evaluation (J, Z)

Evaluates recognition of dynamic gestures using:
- **DTW Similarity**: Dynamic Time Warping for trajectory matching
- **Sequence Accuracy**: Binary correct/incorrect classification
- **Completion Rate**: Gesture completion analysis
- **Threshold**: 85% accuracy required for research-grade

### Real-time Performance Evaluation

Measures system efficiency:
- **FPS Measurement**: Frames per second performance
- **Latency Analysis**: Inference time per frame
- **Hardware Assessment**: CPU/GPU performance
- **Target**: â‰¥ 25 FPS for real-time deployment

## ğŸ”§ Configuration

Edit `config.py` to customize benchmark parameters:

```python
@dataclass
class BenchmarkConfig:
    # Model settings
    model_path: str = "models/yolov11_fingerspelling.pt"
    confidence_threshold: float = 0.5
    
    # Evaluation thresholds
    static_threshold: float = 0.90      # 90% for static letters
    dynamic_threshold: float = 0.85    # 85% for dynamic gestures
    target_fps: float = 25.0           # Target FPS
    dtw_threshold: float = 0.7         # DTW similarity threshold
```

## ğŸ“ˆ API Endpoints

### Prediction Endpoints
- `POST /predict/static/` - Predict static fingerspelling letter
- `POST /predict/dynamic/` - Predict dynamic gesture from frame sequence

### Benchmark Endpoints
- `POST /benchmark/static/` - Run static benchmark
- `POST /benchmark/dynamic/` - Run dynamic benchmark
- `POST /benchmark/efficiency/` - Run efficiency benchmark
- `POST /benchmark/comprehensive/` - Run all benchmarks

### Status Endpoints
- `GET /` - API information
- `GET /config/` - Current configuration
- `GET /benchmark/status/` - Latest benchmark results
- `GET /health/` - Health check

## ğŸ“‹ Dataset Structure

Organize your test dataset as follows:

```
datasets/test_data/
â”œâ”€â”€ A/                    # Static letter A
â”‚   â”œâ”€â”€ image1.jpg
â”‚   â”œâ”€â”€ image2.jpg
â”‚   â””â”€â”€ ...
â”œâ”€â”€ B/                    # Static letter B
â”‚   â””â”€â”€ ...
â”œâ”€â”€ J/                    # Dynamic letter J
â”‚   â”œâ”€â”€ sequence1/        # J gesture sequence
â”‚   â”‚   â”œâ”€â”€ frame1.jpg
â”‚   â”‚   â”œâ”€â”€ frame2.jpg
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ sequence2/
â”‚       â””â”€â”€ ...
â””â”€â”€ Z/                    # Dynamic letter Z
    â”œâ”€â”€ sequence1/
    â””â”€â”€ ...
```

## ğŸ“Š Output Reports

The framework generates comprehensive reports:

### JSON Logs
- Individual prediction logs in `results/logs/`
- Comprehensive benchmark results in `results/`
- Timestamped files for reproducibility

### PDF Reports
- **Full Report**: Detailed analysis with all metrics
- **Summary Report**: Quick overview with key metrics
- **Research Grade Assessment**: Deployment readiness evaluation

## ğŸ” Benchmark Workflow

1. **Data Preparation**: Organize test dataset according to structure
2. **Configuration**: Set thresholds and parameters in `config.py`
3. **Static Evaluation**: Test A-I, K-Y recognition accuracy
4. **Dynamic Evaluation**: Test J, Z gesture recognition with DTW
5. **Performance Testing**: Measure FPS and latency
6. **Report Generation**: Create comprehensive PDF reports
7. **Research Grade Assessment**: Evaluate deployment readiness

## ğŸ“Š Metrics Explained

### Static Metrics
- **Accuracy**: Overall correct predictions / total predictions
- **Precision**: True positives / (True positives + False positives)
- **Recall**: True positives / (True positives + False negatives)
- **F1-Score**: Harmonic mean of precision and recall

### Dynamic Metrics
- **DTW Similarity**: 1 / (1 + DTW_distance) for trajectory matching
- **Sequence Accuracy**: Binary correct/incorrect gesture classification
- **Completion Rate**: Percentage of gesture completion

### Performance Metrics
- **FPS**: Frames processed per second
- **Latency**: Time per inference
- **Real-time Ready**: Meets minimum FPS and latency requirements

## ğŸš¨ Limitations

1. **Alphabet Coverage**: Only fingerspelling A-Z, not full ASL vocabulary
2. **Dataset Dependency**: Accuracy depends on dataset quality and size
3. **Hardware Variation**: Performance varies significantly by CPU/GPU
4. **Dynamic Gestures**: Only J and Z evaluated, not other ASL movements
5. **Generalization**: May not generalize to unseen users or conditions

## ğŸ› ï¸ Troubleshooting

### Common Issues

1. **Model Not Found**: Ensure `yolov11_fingerspelling.pt` is in `models/` directory
2. **Dataset Not Found**: Check dataset structure and paths in `config.py`
3. **Memory Issues**: Reduce batch size or image resolution
4. **Performance Issues**: Check GPU availability and CUDA installation

### Debug Mode

```bash
# Run with verbose output
python benchmarks/benchmark_runner.py --mode comprehensive --verbose
```

## ğŸ“š References

- YOLOv11: [Ultralytics YOLO](https://github.com/ultralytics/ultralytics)
- DTW: Dynamic Time Warping for gesture recognition
- ASL Fingerspelling: American Sign Language alphabet recognition
- Research Grade Criteria: Based on deployment readiness standards

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Add tests for new functionality
4. Submit a pull request

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.

---

**Note**: This benchmark framework is designed for research and development purposes. For production deployment, additional testing and validation may be required.
